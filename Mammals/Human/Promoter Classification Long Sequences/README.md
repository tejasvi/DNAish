# Human Promoter Classification with Long Sequences

This series of notebooks looks at training human promoter classification models for long sequences following the ULMFiT approach. Short here 
is defined as -500/500 relative to known TSS sites. The dataset is constructed following the method outlined in [PromID: Human Promoter Prediction by Deep Learning](https://arxiv.org/pdf/1810.01414.pdf) 
by Umarov et al.  Promoter sequences are generated by locating TSS sites listed in the [EPDnew Database](ftp://ccg.vital-it.ch/epdnew/human/006/) 
and taking the sequence retion -500/500 relative to the TSS. Negative examples are taken from random regions in the genome not containing 
a defined TSS region. The [NCBI Human Genome](https://www.ncbi.nlm.nih.gov/genome/51) is used as a reference template.

Notebook __0__ details preparation of the dataset.

The notebooks in this folder look at training a classification model using the different stages of the ULMFiT process. 

Notebook __1__ trains a naive baseline model. This model trains from scratch using only the promoter sequences dataset.

Notebook __2__ trains the classification model initialized with a pre-trained [human genome language model](https://github.com/tejasvi/DNAish/tree/master/Mammals/Human/Genomic%20Language%20Models).

Notebook __3__ first fine tunes the human genome language model on the promoter corpus, then trains a classification model intitialized with 
the fine tuned language model. The 5-mer stride 2 language model is used.

Notebook __4__ follows the same training procedure to Notebook __3__, except it uses the 4-mer stride 2 language model.

Notebook __5__ follows the same training procedure as Notebook __3__ and Notebook __4__, using the 8-mer stride 3 language model.

Notebook __6__ trains a 1-mer stride 1 model

Results compared to Umarov et al.:

| Model                                   	| DNA Size  	| Kmer/Stride 	| Models           	| Accuracy 	| Precision 	| Recall 	| Correlation Coefficient 	|
|-----------------------------------------	|-----------	|-------------	|------------------	|----------	|-----------	|--------	|-------------------------	|
| Umarov et al.                           	| -1000/500 	|      -      	| 2 Model Ensemble 	|     -    	|   0.636   	|  0.802 	|          0.714          	|
| Umarov et al.                           	|  -200/400 	|      -      	| 2 Model Ensemble 	|     -    	|   0.769   	|  0.755 	|          0.762          	|
| Naive Model                             	|  -500/500 	|     5/2     	|   Single Model   	|   0.858  	|   0.877   	|  0.772 	|          0.708          	|
| With Pre-Training                        	|  -500/500 	|     5/2     	|   Single Model   	|   0.888  	|   __0.902__   	|  0.824 	|          0.770          	|
| With Pre-Training and Fine Tuning (5mer) 	|  -500/500 	|     5/2     	|   Single Model   	|   0.889  	|   0.886   	|  0.846 	|          0.772          	|
| With Pre-Training and Fine Tuning (4mer) 	|  -500/500 	|     4/2     	|   Single Model   	|   0.892  	|   0.877   	|  __0.865__ 	|          0.778          	|
| With Pre-Training and Fine Tuning (8mer) 	|  -500/500 	|     8/3     	|   Single Model   	|   0.874  	|   0.889   	|  0.802 	|          0.742          	|
| With Pre-Training and Fine Tuning (1mer) 	|  -500/500 	|     1/1     	|   Single Model   	|   __0.894__  	|   0.900   	|  0.844 	|          __0.784__          	|
