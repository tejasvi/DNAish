{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E. coli Promoter Classification Naive Model\n",
    "\n",
    "In this notebook we train a classification model using only the promoter dataset - no language model pretraining. This serves as a baseline to verify the effect of unsupervised pretraining.\n",
    "\n",
    "This notebook also elaborates on how genomic data is process for the classification model. Many functions in the `utils.py` file are explained here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.text import *\n",
    "from Bio import Seq\n",
    "from Bio.Seq import Seq\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.SeqFeature import FeatureLocation, CompoundLocation\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../..\")\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('F:/genome/e_coli/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_df = pd.read_csv(path/'e_coli_promoters_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only input to the model will be sequence data from -150/50 from defined TSS locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gene</th>\n",
       "      <th>Locus</th>\n",
       "      <th>Location</th>\n",
       "      <th>Sample Location</th>\n",
       "      <th>Orientation</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Promoter</th>\n",
       "      <th>Independent</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['mokC']</td>\n",
       "      <td>['b0018']</td>\n",
       "      <td>[16750:16960](-)</td>\n",
       "      <td>[16910:17060](-)</td>\n",
       "      <td>reverse</td>\n",
       "      <td>TAGCGGCGGGTGCTTGAGGCTGTCTGTCTCAGGCATTAGCTGAACG...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['insB-1']</td>\n",
       "      <td>['b0021']</td>\n",
       "      <td>[19810:20314](-)</td>\n",
       "      <td>[20264:20414](-)</td>\n",
       "      <td>reverse</td>\n",
       "      <td>GCTCTCACTGCCGTAAAACATGGCAACTGCAGTTCACTTACACCGC...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['insA-1']</td>\n",
       "      <td>['b0022']</td>\n",
       "      <td>[20232:20508](-)</td>\n",
       "      <td>[20458:20608](-)</td>\n",
       "      <td>reverse</td>\n",
       "      <td>GACTCCCCCACAAAGAATATGGATATTGTGATACACATTGAGGTAG...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['rpsT']</td>\n",
       "      <td>['b0023']</td>\n",
       "      <td>[20814:21078](-)</td>\n",
       "      <td>[21028:21178](-)</td>\n",
       "      <td>reverse</td>\n",
       "      <td>ACGGCGCTTATTTGCACAAATCCATTGACAAAAGAAGGCTAAAAGG...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['fkpB']</td>\n",
       "      <td>['b0028']</td>\n",
       "      <td>[25825:26275](+)</td>\n",
       "      <td>[25725:25875](+)</td>\n",
       "      <td>forward</td>\n",
       "      <td>ACGCATCTTATCCGGCCTACAGATTGCTGCGAAATCGTAGGCCGGA...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Gene      Locus          Location   Sample Location Orientation  \\\n",
       "0    ['mokC']  ['b0018']  [16750:16960](-)  [16910:17060](-)     reverse   \n",
       "1  ['insB-1']  ['b0021']  [19810:20314](-)  [20264:20414](-)     reverse   \n",
       "2  ['insA-1']  ['b0022']  [20232:20508](-)  [20458:20608](-)     reverse   \n",
       "3    ['rpsT']  ['b0023']  [20814:21078](-)  [21028:21178](-)     reverse   \n",
       "4    ['fkpB']  ['b0028']  [25825:26275](+)  [25725:25875](+)     forward   \n",
       "\n",
       "                                            Sequence  Promoter  Independent  \\\n",
       "0  TAGCGGCGGGTGCTTGAGGCTGTCTGTCTCAGGCATTAGCTGAACG...         1        False   \n",
       "1  GCTCTCACTGCCGTAAAACATGGCAACTGCAGTTCACTTACACCGC...         1        False   \n",
       "2  GACTCCCCCACAAAGAATATGGATATTGTGATACACATTGAGGTAG...         1        False   \n",
       "3  ACGGCGCTTATTTGCACAAATCCATTGACAAAAGAAGGCTAAAAGG...         1        False   \n",
       "4  ACGCATCTTATCCGGCCTACAGATTGCTGCGAAATCGTAGGCCGGA...         1        False   \n",
       "\n",
       "     set  \n",
       "0  train  \n",
       "1  train  \n",
       "2  train  \n",
       "3  train  \n",
       "4  train  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = classification_df[classification_df.set == 'train']\n",
    "valid_df = classification_df[classification_df.set == 'valid']\n",
    "test_df = classification_df[classification_df.set == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6791, 9), (750, 9), (830, 9))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, valid_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and Numericalization\n",
    "\n",
    "Before feeding genomic sequence data to the model, it needs to be processed into tokens. For this, we use the `GenomicTokenizer` class. This subclasses the `BaseTokenizer` class in the fastai library.The genomic sequences are tokenized by breaking the sequence into k-mers. The k-mer tokenization is defined by two parameters - `ngram` and `stride`. `ngram` determines the length of the k-mers. `stride` defines how far the k-mer window moves between k-mers. Empirically I have found that a k-mer size of 5 with a stride of 2 works well. This creates a list of sequential tokens with 3bp of overlap between each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenomicTokenizer(BaseTokenizer):\n",
    "    def __init__(self, lang='en', ngram=5, stride=2):\n",
    "        self.lang = lang\n",
    "        self.ngram = ngram\n",
    "        self.stride = stride\n",
    "        \n",
    "    def tokenizer(self, t):\n",
    "        t = t.upper()\n",
    "        if self.ngram == 1:\n",
    "            toks = list(t)\n",
    "        else:\n",
    "            toks = [t[i:i+self.ngram] for i in range(0, len(t), self.stride) if len(t[i:i+self.ngram]) == self.ngram]\n",
    "        if len(toks[-1]) < self.ngram:\n",
    "            toks = toks[:-1]\n",
    "        \n",
    "        return toks\n",
    "    \n",
    "    def add_special_cases(self, toks):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenization function is passed to the fastai `Tokenizer` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer(GenomicTokenizer, n_cpus=1, pre_rules=[], post_rules=[], special_cases=['xxpad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once tokens are created, they are numericalized with the `GenomicVocab` class. This class holds two objects: `itos`, a list of each k-mer in the model's vocabulary, and `stoi`, a dictionary that maps each k-mer to its index in the `itos` list. This creates the k-mer to number mapping that we will use to numericalize our data. When we tokenize the sequences into 5bp tokens, we create a vocabulary of 1024 (4^5) tokens.  We also insert a padding token that will allow us to deal with sequences of different length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenomicVocab(Vocab):\n",
    "    def __init__(self, itos):\n",
    "        self.itos = itos\n",
    "        self.stoi = collections.defaultdict(int,{v:k for k,v in enumerate(self.itos)})\n",
    "        \n",
    "    @classmethod\n",
    "    def create(cls, tokens, max_vocab, min_freq):\n",
    "        freq = Counter(p for o in tokens for p in o)\n",
    "        itos = [o for o,c in freq.most_common(max_vocab) if c >= min_freq]\n",
    "        itos.insert(0, 'pad')\n",
    "        return cls(itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing and numericalizing are handled by the `PreProcessor` class. Specifically we have the `GenomicTokenizeProcessor` for tokenization and the `GenomicNumericalizeProcessor` for numericalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenomicTokenizeProcessor(PreProcessor):\n",
    "    \"`PreProcessor` that tokenizes the texts in `ds`.\"\n",
    "    def __init__(self, ds:ItemList=None, tokenizer:Tokenizer=None, chunksize:int=10000, mark_fields:bool=False):\n",
    "        self.tokenizer,self.chunksize,self.mark_fields = ifnone(tokenizer, Tokenizer()),chunksize,mark_fields\n",
    "\n",
    "    def process_one(self, item):  \n",
    "        return self.tokenizer._process_all_1(_genomic_join_texts([item], self.mark_fields))[0]\n",
    "    \n",
    "    def process(self, ds):\n",
    "        ds.items = _genomic_join_texts(ds.items, self.mark_fields)\n",
    "        tokens = []\n",
    "        for i in range(0,len(ds),self.chunksize):\n",
    "            tokens += self.tokenizer.process_all(ds.items[i:i+self.chunksize])\n",
    "        ds.items = tokens\n",
    "        \n",
    "class GenomicNumericalizeProcessor(PreProcessor):\n",
    "    \"`PreProcessor` that numericalizes the tokens in `ds`.\"\n",
    "    def __init__(self, ds:ItemList=None, vocab:Vocab=None, max_vocab:int=60000, min_freq:int=3):\n",
    "        vocab = ifnone(vocab, ds.vocab if ds is not None else None)\n",
    "        self.vocab,self.max_vocab,self.min_freq = vocab,max_vocab,min_freq\n",
    "\n",
    "    def process_one(self,item): return np.array(self.vocab.numericalize(item), dtype=np.int64)\n",
    "    def process(self, ds):\n",
    "        if self.vocab is None: self.vocab = GenomicVocab.create(ds.items, self.max_vocab, self.min_freq)\n",
    "        ds.vocab = self.vocab\n",
    "        super().process(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these functions to process the input dataframe is handled by the `GenomicTextClasDataBunch` class. This takes as input the training data, the tokenizer, and a vocabulary if one is predefined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenomicTextClasDataBunch(TextClasDataBunch):\n",
    "    @classmethod\n",
    "    def from_df(cls, path:PathOrStr, train_df:DataFrame, valid_df:DataFrame, test_df:Optional[DataFrame]=None,\n",
    "                tokenizer:Tokenizer=None, vocab:Vocab=None, classes:Collection[str]=None, text_cols:IntsOrStrs=1,\n",
    "                label_cols:IntsOrStrs=0, label_delim:str=None, chunksize:int=10000, max_vocab:int=60000,\n",
    "                min_freq:int=2, mark_fields:bool=False, pad_idx=0, pad_first=True, bs=64, **kwargs) -> DataBunch:\n",
    "        \"Create a `TextDataBunch` from DataFrames. `kwargs` are passed to the dataloader creation.\"\n",
    "        processor = _get_genomic_processor(tokenizer=tokenizer, vocab=vocab, chunksize=chunksize, max_vocab=max_vocab,\n",
    "                                   min_freq=min_freq, mark_fields=mark_fields)\n",
    "        if classes is None and is_listy(label_cols) and len(label_cols) > 1: classes = label_cols\n",
    "        src = ItemLists(path, TextList.from_df(train_df, path, cols=text_cols, processor=processor),\n",
    "                        TextList.from_df(valid_df, path, cols=text_cols, processor=processor))\n",
    "        src = src.label_from_df(cols=label_cols, classes=classes, label_delim=label_delim)\n",
    "        if test_df is not None: src.add_test(TextList.from_df(test_df, path, cols=text_cols))\n",
    "        d1 = src.databunch(**kwargs)\n",
    "        \n",
    "        datasets = cls._init_ds(d1.train_ds, d1.valid_ds, d1.test_ds)\n",
    "        collate_fn = partial(pad_collate, pad_idx=pad_idx, pad_first=pad_first, backwards=False)\n",
    "        train_sampler = SortishSampler(datasets[0].x, key=lambda t: len(datasets[0][t][0].data), bs=bs//2)\n",
    "        train_dl = DataLoader(datasets[0], batch_size=bs, sampler=train_sampler, drop_last=True, **kwargs)\n",
    "        dataloaders = [train_dl]\n",
    "        for ds in datasets[1:]:\n",
    "            lengths = [len(t) for t in ds.x.items]\n",
    "            sampler = SortSampler(ds.x, key=lengths.__getitem__)\n",
    "            dataloaders.append(DataLoader(ds, batch_size=bs, sampler=sampler, **kwargs))\n",
    "            \n",
    "        return cls(*dataloaders, path=path, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas = GenomicTextClasDataBunch.from_df(path, train_df, valid_df, test_df=test_df, tokenizer=tok, \n",
    "                                            text_cols='Sequence', label_cols='Promoter', bs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "As discussed, the genomic sequences for the promoters are split into 5-mers with a stride of 2. Here we can see the raw tokens and their numericalized form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Text TAGCG GCGGC GGCGG CGGGT GGTGC TGCTT CTTGA TGAGG AGGCT GCTGT TGTCT TCTGT TGTCT TCTCA TCAGG AGGCA GCATT ATTAG TAGCT GCTGA TGAAC AACGG CGGCA GCAGA AGATA ATAGA AGAGA AGAAA AAAAG AAGCC GCCCC CCCGA CGAGT AGTGA TGATA ATATT ATTTT TTTAC TACCA CCATC ATCAA CAACC ACCCG CCGAG GAGGC GGCCT CCTCC TCCTA CTATA ATATG ATGCT GCTGA TGAAC AACAC CACAT CATGT TGTAG TAGAG GAGTG GTGCC GCCTC CTCTT CTTAC TACTG CTGAC GACCG CCGTA GTAAG AAGGT GGTCA TCAAG AAGGA GGAGA,\n",
       " Category 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clas.train_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the integers associated with each k-mer. When these integers are fed to the model, they map to specific rows of the embedding matrix. The embedding contains vector representations of each k-mer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([678,  73,  36, 694, ..., 435, 917, 334, 464], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clas.train_ds[0][0].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Model\n",
    "\n",
    "The model used is based on the [AWD-LSTM](https://arxiv.org/abs/1708.02182) model. The model consists of an embedding layer, followed by a stack of LSTM layers, followed by several linear layers. The configuration I typically use is defined by the `clas_config` dictionary below. \n",
    "\n",
    "The embedding vectors are of length 400. The LSTM section consists of three stacked layers with a hidden size of 1150 units. The final linear section is defined by the `PoolingLinearClassifier` class in the fastai library. The output from the LSTM layers contain each hidden state from each k-mer. The linear section takes the final hidden state, as well as the max pooling vector over the hidden states and the average pooling vector over the hidden states. These three vectors are concatenated together and sent through two standard linear layers. The output is a vector of length 2, representing probabilities of the input being a promoter or not a promoter.\n",
    "\n",
    "Regularization is applied to the model following the implementation in the AWD-LSTM paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clas_config = dict(emb_sz=400, n_hid=1150, n_layers=3, pad_token=0, qrnn=False, output_p=0.4, \n",
    "                       hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5)\n",
    "drop_mult = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_model_clas(data_clas, drop_mult, clas_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(1025, 400, padding_idx=0)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(1025, 400, padding_idx=0)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1150, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1150, 1150, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1150, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.08000000000000002)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained for several epochs following the learning rate and momentum scheduling outlined in [A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay](https://arxiv.org/abs/1803.09820)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n",
      "Min numerical gradient: 2.09E-03\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 00:46 <p><table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>0.592741</th>\n",
       "    <th>0.692581</th>\n",
       "    <th>0.500000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>0.464481</th>\n",
       "    <th>0.640619</th>\n",
       "    <th>0.542667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3</th>\n",
       "    <th>0.402478</th>\n",
       "    <th>0.352550</th>\n",
       "    <th>0.853333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>4</th>\n",
       "    <th>0.348807</th>\n",
       "    <th>0.313955</th>\n",
       "    <th>0.864000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>5</th>\n",
       "    <th>0.295296</th>\n",
       "    <th>0.294537</th>\n",
       "    <th>0.866667</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(5, 2e-3, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 00:46 <p><table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>0.197571</th>\n",
       "    <th>0.316365</th>\n",
       "    <th>0.865333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>0.215816</th>\n",
       "    <th>0.340545</th>\n",
       "    <th>0.860000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3</th>\n",
       "    <th>0.206282</th>\n",
       "    <th>0.379665</th>\n",
       "    <th>0.845333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>4</th>\n",
       "    <th>0.185619</th>\n",
       "    <th>0.353171</th>\n",
       "    <th>0.853333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>5</th>\n",
       "    <th>0.161763</th>\n",
       "    <th>0.342562</th>\n",
       "    <th>0.864000</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(5, 5e-4, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 00:46 <p><table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>0.123969</th>\n",
       "    <th>0.342915</th>\n",
       "    <th>0.866667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>0.122340</th>\n",
       "    <th>0.403346</th>\n",
       "    <th>0.845333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3</th>\n",
       "    <th>0.117600</th>\n",
       "    <th>0.386390</th>\n",
       "    <th>0.853333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>4</th>\n",
       "    <th>0.111036</th>\n",
       "    <th>0.370066</th>\n",
       "    <th>0.868000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>5</th>\n",
       "    <th>0.103370</th>\n",
       "    <th>0.376861</th>\n",
       "    <th>0.860000</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(5, 1e-4, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('coli_naive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the trained model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas = GenomicTextClasDataBunch.from_df(path, train_df, test_df, tokenizer=tok, \n",
    "                                            text_cols='Sequence', label_cols='Promoter', bs=300)\n",
    "learn.data = data_clas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8349397590361446\n",
      "False Positives: 0.07349397590361446\n",
      "False Negatives: 0.09156626506024096\n",
      "Recall: 0.8168674698795181\n",
      "Precision: 0.8475\n",
      "MCC: 0.6703175228168727\n"
     ]
    }
   ],
   "source": [
    "get_scores(learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model achieves good accuracy, precision, and recall, but can be extremely unstable. Observe how the accuracy can fluctuate sharply between epochs. Several runs were needed to get the results above. Bad initializations result in the model never actually training. In the next notebook, we will see how pretraining can significantly stabilize the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
