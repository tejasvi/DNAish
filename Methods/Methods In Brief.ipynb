{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ULMFiT for Genomics Classification\n",
    "\n",
    "Karl Heyer\n",
    "\n",
    "## Abstract\n",
    "\n",
    "Classifying functional genomic regions such as promoters or enhancers from sequence data alone is a notoriously difficult challenge. Many bioinformatics algorithms exist for genomics classification, but these tend to rely on hand engineered features and are limited to recognizing known structural motifs. More recently, deep learning approaches developed in computer vision (CV) and natural language processing (NLP) have been applied to genomics problems with promising results [26]. However, these methods tend to be limited by the amount of labeled data available. We present Genomic-ULMFiT, an application of Universal Language Model for Fine-tuning (ULMFiT) [1] to genomics classification problems. ULMFiT is an effective NLP algorithm for inductive transfer learning that utilizes pre-training on large unlabeled corpuses. This is particularly advantageous in the context of genomics data, which tends to contain significant volumes of unlabeled data. Genomic-ULMFiT shows improved results over existing methods for classification of promoters, enhancers, taxonomy, and lncRNA using only sequence data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Advancements in genomics research such as high-throughput sequencing have driven an explosion of genomic data. The mountain of data demands robust algorithms to analyze and interpret the data. Genomic classification - asigning genomic sequences a label based on function or some other property - is fundamental to processing raw genomics data. Algorithms exist to classify sequences based on hand engineered features, but these approaches struggle with the structural heterogeneity of genomic sequences [6]. Structures of genomic regulatory features such as promoters are notoriously complex and diverse [32].\n",
    "\n",
    "Recently deep learning approaches have tried to leverage large genomics datasets to train end-to-end models to resolve the complex structural diversity of genomic sequences. In the context of classification problems, deep learning has been applied to promoter classification [5,6], enhancer classification [3,4,7,12], enhancer-promoter interactions [8], CRISPR guide scoring [2], transcription factor binding sites [9], metagenomics classification [10], delitrious mutation classification [11], and long noncoding RNA classification [27]. One limitation of these methods as implemented is they all rely on labled data. Even when techniques like pre-training and transfer learning are used, such steps are applied only to labeled classification corpuses [8,9,12,28,29,30]. This poses a problem, as labeled genomics datasets tend to be small and deep learning models are prone to overfitting on small datasets [31].\n",
    "\n",
    "ULMFiT is a NLP algorithm for training text classification models using transfer learning. ULMFiT uses unsupervised pre-training on large unlabeled domain corpuses to improve classification on smaller labeled datasets. This approach has shown to reduce error rate on common text classification tasks by 18-24% over training from scratch, while also reducing data requirements by up to 100x [1]. In the context of genomic data, ULMFiT allows us to pre-train models on large amounts of unlabeled sequence data, then use the pre-trained models for classification on smaller corpuses of labeled data. This helps address the limitations caused by labeled sequence availability.\n",
    "\n",
    "## Contributions\n",
    "\n",
    "We adapt ULMFiT to genomics data with k-mer based tokenization. We show that Genomic-ULMFiT produces superior classification results in several genomic contexts, improving metrics such as accuracy, precision and recall by up to 18%. We provide a roadmap for approaching genomics classification problems in any genomic context using unsupervised pre-training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Related Work\n",
    "\n",
    "\n",
    "### Deep Learning in Genomics\n",
    "\n",
    "Deep learning has been applied to many areas of genomics for some time now [26]. Deep classification models have been applied to a variety of genomics classification problems such as promoter classification [5,6], enhancer classification [3,4,7,12], enhancer-promoter interactions [8], CRISPR guide scoring [2], transcription factor binding sites [9], metagenomics classification [10], delitrious mutation classification [11], and long noncoding RNA classification [27] to name a few. Deep models for genomics typically use CNN based architectures [2,3,5,6,7,10] or GRU cell architectures [4,9,27]. \n",
    "\n",
    "### Text Representation of Genomic Sequences\n",
    "\n",
    "An important design decision in training deep genomics models is choosing how to represent genomic sequences as a numerical input to the model. Genomic sequences are typically tokenized on the nucleotide level [2,3,4,5,6,7,27]. Some approaches tokenize on the k-mer level [8,9,10]. Tokenized sequences are then represented either as one hot encoded vectors [2,3,4,5,6,7,10] or passed through an embedding layer [8,9,27].\n",
    "\n",
    "\n",
    "### Language Modeling\n",
    "\n",
    "Language modeling is the task of taking in as input a sequence of tokens and predicting the next token in the sequence. Language modeling in NLP has proven to be a powerful pre-training tool for transfer learning. Language modeling as a task trains a model to learn long term dependencies, hierarchical relations and sentiment [1]. Language models have shown the ability to be fine-tuned to many different classification tasks related to the language domain the language model was trained on [1].\n",
    "\n",
    "\n",
    "### ULMFiT\n",
    "\n",
    "ULMFiT is an inductive transfer learning technique in NLP [1]. ULMFiT trains a text classification model in three steps. The first step is general domain language modeling. A language model is trained on a large, general, unlabeled corpus that shares domain similarities to the classification task. The second step is to fine tune the general language model on the classification corpus. The third step is to use the pre-trained weights of the fine tuned language model to train the final classification model on the classification corpus. The ULMFiT approach has shown to train better performing classification models requiring smaller amounts of labeled data compared to training from scratch [1].\n",
    "\n",
    "### Regularization of Language Models\n",
    "\n",
    "Language models can be prone to overfitting. [13] showed that using a variety of effective regularization techniques is crucial for training high quality language models that do not overfit. Dropout [17] is applied in four different ways. Embedding dropout is the equivalent of applying dropout on the word level, broadcasting dropout along the entire embedding vector for a word with a given probability. Weight dropout, similar to DropConnect [19] is applied to the hidden-to-hidden matrices of the LSTM layers of the model. For weight drop, a constant dropout mask is applied at all timesteps to avoid issues raised by [18]. Variational dropout is applied to the activations of the hidden layers, also using a constant dropout mask at all timesteps. Standard dropout is used on linear layers after LSTM layers. Two forms of activation regularization are also applied. Activation Regularization (AR) applies an L2 scale penalty to the activations in the LSTM layers of the model. Temporal Activation Regularization (TAR) [21] applies an L2 penalty to the change in activation scale between timesteps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Genomic ULMFiT\n",
    "\n",
    "We propose Genomic ULMFiT - an adaptation of the ULMFiT training process to genomic data. Following the ULMFiT process, we first train a general domain genomic language model. This genomic language model takes in a sequence of genomic tokens and predicts the next token in the sequence. The genomic language model is then fine tuned on a target classification corpus to create a fine tuned genomic language model. The weights of the fine tuned genomic language model are then transferred to a classification model that is trained on the target classification corpus.\n",
    "\n",
    "### Datasets\n",
    "\n",
    "Two types of datasets are used in the ULMFiT process. The first is the dataset used for training the general domain genomic language model. The second is the dataset used for the target classification task.\n",
    "\n",
    "Training the general domain language model is a self-supervised process. The model takes in a sequence of tokens and predicts the next token. This training stage required no labeled data. This allows us to take advantage of the large amounts of unlabeled genomic data available. To create datasets for training the general domain genomic language model, full genomes are downloaded from NCBI. This makes it easy to generate large genomic datasets. The dataset used for pre-training should be phylogenically similar to the target classification task the model will be transferred to. If for example the target task was classifying human promoter sequences, the general domain genomic language model should be trained on human genomic data or genomic data from closely related species.\n",
    "\n",
    "Datasets used for classification tasks are taken from existing publications for comparison. This is discussed more in the results section. In short, classification datasets are taken from [5,6,7,10,27].\n",
    "\n",
    "### Data Representation\n",
    "\n",
    "Genomic data must be pre-processed and tokenized before being input to the model. Tokenization on the k-mer level following [8,9,10] leads to better performance than nucleotide level tokenization used by [2,3,4,5,6,7,27]. k-mer level tokenization allows the language model to learn a more nuanced understanding of different k-mers compared to nucleotide level tokenization.\n",
    "\n",
    "Tokenization is also done with a set stride between tokens that may be less than the k-mer value. For example consider tokenizing the sequence `ATCGCGTACGATCCG` with a k-mer size of 4 and the following stride values:\n",
    " * Stride 1: `ATCG TCGC CGCG GCGT CGTA GTAC TACG ACGA CGAT GATC ATCC TCCG`\n",
    " * Stride 2: `ATCG CGCG CGTA TACG CGAT ATCC`\n",
    " * Stride 3: `ATCG GCGT TACG GATC`\n",
    " * Stride 4: `ATCG CGTA CGAT`\n",
    "\n",
    "Decreasing the stride between k-mers can be thought of as applying a prior to the language model prediction process where information about the next k-mer is included in the previous k-mer. This allows the language model to converge more quickly to a lower loss value, but this does not always translate to better classification performance.\n",
    "\n",
    "In practice, tokenization parameters for the length of k-mers used and the stride between k-mers are hyperparameters that must be tuned for different datasets. Some datasets respond strongly to the choice of k-mer and stride, while other datasets show no performance change for a range of k-mer and stride values.\n",
    "\n",
    "### General Domain Genomic Language Model\n",
    "\n",
    "The general domain genomic language model is based on the AWD-LSTM model [13] which is the standard model for ULMFiT [1]. This model utilizes LSTM cells for procesing sequences [14]. The model is regularized following [13]. All forms of dropout described in [13] are used - variational dropout, weight-drop, embedding dropout and standard dropout. Activation regularization, temporal activation regularization and weight decay are also used. Weight decay is implemented following [20] where the weight decay penalty is added in the weight update step rather than in the loss calculation. The model uses weight tying between the embedding layer and the output layer, motivated by [15,16].\n",
    "\n",
    "The general domain genomic language model is trained in a self-supervised fashion to predict the next genomic token in a series of genomic tokens. Typical datasets include an organism's genome, or an ensemble of genomes from closely related organisms. The model is trained following the One Cycle learning rate policy from [22]. Learning rates are selected following the methods of [23,24]. The model is optimized using the Adam optimizer with parameters $\\beta_{1} = 0.9, \\beta_{2} = 0.99$.\n",
    "\n",
    "### Task Specific Genomic Language Model\n",
    "\n",
    "No matter how good the general domain genomic language model is, the classification dataset likely comes from a different distribution [1]. This motivates fine-tuning the genomic language model on the classification corpus. Following [1,25], the genomic language model is fine-tuned using discriminative fine-tuning where different layers in the model receive different learning rates and are fine-tuned to different extents. In this stage, regularization hyperparameters may need to be changed. The model is still trained following the One Cycle learning rate policy.\n",
    "\n",
    "### Task Specific Classification Model\n",
    "\n",
    "Weights from the fine-tuned genomic langauge model are then transferred to the classification model. The classification model adds two new linear layers on top of the genomic language model. Following [1], the linear layers use batch normalization, dropout and ReLU activations. The linear layers use the concat pooling method described in [1].\n",
    "\n",
    "The classification model is trained using gradual unfreezing. Training all layers of the classification model at once could lead to catastrophic forgetting. Training all layers also risks allowing a bad gradient update in the new, untrained linear layers to dramatically change the pre-trained weights in the lower layers of the model. With gradual unfreezing, we first train only the new linear layers at the end of the model. Then we unfreeze one layer at a time over several training steps.\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "Models typically include an embedding of size 400, three LSTM layers with 1150 hidden activations per layer, and a BPTT value of 70. Typical dropout parameters are 0.1 for embeddings, 0.5 for the LSTM hidden-to-hidden matrix, 0.2 to the activations of the LSTM layers and 0.4 to the output linear layers. Activation regularization and temporal activation regularization are implemented with parameters $\\alpha = 2$, $\\beta = 1$. The weight decay coefficient is typically 0.01. Dropout and weight decay are tuned on a per-dataset basis. Typically dropout parameters are tuned by applying a constant scaling coefficient, maintaining the ratios between dropout parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### E. coli Baseline\n",
    "\n",
    "This is a baseline comparison to show the effect of pre-training and validate that the Genomic-ULMFiT approach improves results over training from scratch. Here the Naive model is trained from scratch. The E. coli Genome Pre-Training model is pre-trained on only the E. coli genome. The Genomic Ensemble Pre-Training model is trained on a dozen or so bacterial genomes. Pre-training has a clear impact on model performance. Pre-training on more data shows improvements over pre-training on less data. In general the quality of the pre-trained language model has a direct impact on classification performance.\n",
    "\n",
    "  | Model                        \t| Accuracy \t| Precision \t| Recall \t| Correlation Coefficient \t|\n",
    "  |------------------------------\t|:--------:\t|:---------:\t|:------:\t|:-----------------------:\t|\n",
    "  | Naive                        \t|   0.834  \t|   0.847   \t|  0.816 \t|          0.670          \t|\n",
    "  | E. coli Genome Pre-Training   \t|   0.919  \t|   0.941   \t|  0.893 \t|          0.839          \t|\n",
    "  | Genomic Ensemble Pre-Training \t|   0.973  \t|   0.980   \t|  0.966 \t|          0.947          \t|\n",
    "\n",
    "\n",
    "### Human Promoters, Short Sequences\n",
    "\n",
    "This data shows a direct comparison to [5] for classification of human promoters from short (250 bp) sequences, taken -200/50 relative to the TSS. The same dataset from [5] was used to generate these results.\n",
    "\n",
    "\n",
    "| Model                            \t| Accuracy \t| Precision \t| Recall \t| Correlation Coefficient \t| Specificity \t|\n",
    "|----------------------------------\t|----------\t|-----------\t|--------\t|-------------------------\t|-------------\t|\n",
    "| Kh et al.                        \t|     -    \t|     -     \t|   0.9  \t|           0.89          \t|     0.98    \t|\n",
    "| Genomic-ULMFiT \t|   .995   \t|    .992   \t|  __.996__  \t|           __.991__          \t|     __.994__    \t|\n",
    "\n",
    "\n",
    "\n",
    "### Human Promoters, Long Sequences\n",
    "\n",
    "These results show a direct comparison to [6]. The dataset for [6] was not publicly available, but the same methodology was used to generate a dataset. Positive sequences were taken as the region -500/500 relative to TSS locations in the [EPDnew Database](https://epd.epfl.ch//EPDnew_database.php). Negative sequences were randomly selected from regions in the genome not overlapping with regions taken for promoter sequences. The [NCBI Human Genome](https://www.ncbi.nlm.nih.gov/genome/51) is used as a reference template.\n",
    "\n",
    "| Model                                   \t| DNA Size  \t| Models           \t| Accuracy \t| Precision \t| Recall \t| Correlation Coefficient \t|\n",
    "|-----------------------------------------\t|-----------\t|------------------\t|----------\t|-----------\t|--------\t|-------------------------\t|\n",
    "| Umarov et al.                           \t| -1000/500 \t| 2 Model Ensemble \t|     -    \t|   0.636   \t|  0.802 \t|          0.714          \t|\n",
    "| Umarov et al.                           \t|  -200/400 \t| 2 Model Ensemble \t|     -    \t|   0.769   \t|  0.755 \t|          0.762          \t|\n",
    "| Genomic-ULMFiT\t|  -500/500 \t|   Single Model   \t|   0.894  \t|   __0.900__   \t|  __0.844__ \t|          __0.784__          \t|\n",
    "\n",
    "\n",
    "### Bacterial Promoters\n",
    "\n",
    "These results show comparisons to performance on another dataset from [5] containing promoter sequences from E. coli and B. subtilis. Compared to the CNN-based method used by [5], Genomic-ULMFiT performed similarly on E. coli promoters, but worse on B. subtilis promoters, likely due to the amount of data available (2936 examples for E. coli, 1050 for B. subtilis). This suggests that in extremely low data regimes, CNN models may perform better.\n",
    "\n",
    "\n",
    "| Method         \t| Organism    \t| Training Examples \t| Accuracy \t| Precision \t| Recall \t| Correlation Coefficient \t| Specificity \t|\n",
    "|----------------\t|-------------\t|-------------------\t|----------\t|-----------\t|--------\t|-------------------------\t|-------------\t|\n",
    "| Kh et al.     \t| E. coli     \t|        2936       \t|     -    \t|     -     \t|  __0.90__  \t|           0.84          \t|     0.96    \t|\n",
    "| Genomic-ULMFiT \t| E. coli     \t|        2936       \t|   0.956  \t|   0.917   \t|  0.880 \t|          __0.871__          \t|    __0.977__    \t|\n",
    "| Kh et al.     \t| B. subtilis \t|        1050       \t|     -    \t|     -     \t|  __0.91__  \t|           __0.86__          \t|     0.95    \t|\n",
    "| Genomic-ULMFiT \t| B. subtilis \t|        1050       \t|   0.905  \t|   0.857   \t|  0.789 \t|          0.759          \t|     0.95    \t|\n",
    "\n",
    "\n",
    "### Metagenomics Classification\n",
    "\n",
    "These results show a direct comparison to [10], using the same datasets for classification. Two datasets are used - one for amplicon sequencing data, another for shotgun sequencing data. Datasets are generated synthetically based on sequencing of S16 regions of bacterial genomes.\n",
    "\n",
    "\n",
    "| Amplicon Data   \t| Accuracy \t| Precision \t| Recall \t| F1    \t|\n",
    "|-----------------\t|----------\t|-----------\t|--------\t|-------\t|\n",
    "| Fiannaca et al. \t|   .9137  \t|   .9162   \t|  .9137 \t| .9126 \t|\n",
    "| Genomic-ULMFiT  \t|   __.9239__  \t|   __.9402__   \t|  __.9332__ \t| __.9306__ \t|\n",
    "\n",
    "| Shotgun Data    \t| Accuracy \t| Precision \t| Recall \t| F1    \t|\n",
    "|-----------------\t|----------\t|-----------\t|--------\t|-------\t|\n",
    "| Fiannaca et al. \t|   .8550  \t|   .8570   \t|  .8520 \t| .8511 \t|\n",
    "| Genomic-ULMFiT  \t|   __.8797__  \t|   __.8824__   \t|  __.8769__ \t| __.8758__ \t|\n",
    "\n",
    "\n",
    "### Enhancer Classification\n",
    "\n",
    "These results show a direct comparison to [7], using the same dataset. Results here are compared using ROC-AUC as this was the metric used by [7]. Positive examples are \n",
    "500 bp sequences defined as having active enhancer marks (H3K27ac) in the liver. Negative examples are genomic regions showing no H3K27ac marks.\n",
    "\n",
    "The data from [7] on this dataset is actually not presented in the paper itself, but put in the supplementary section, available [here](https://www.biorxiv.org/content/biorxiv/suppl/2018/02/14/264200.DC2/264200-1.pdf). The results below are compared to the author's results in supplementary Figure 3. This dataset was used because the main dataset from [7] used to generate figured in the main paper was not made available on their github repo.\n",
    "\n",
    "| Model/ROC-AUC                 \t| Human \t| Mouse \t|  Dog  \t| Opossum \t|\n",
    "|-------------------------------\t|:-----:\t|:-----:\t|:-----:\t|:-------:\t|\n",
    "| Cohn et al.                   \t|  0.80 \t|  0.78 \t|  0.77 \t|   0.72  \t|\n",
    "| Genomic-ULMFiT \t| __0.819__ \t| __0.875__ \t| __0.788__ \t|  __0.798__  \t|\n",
    "\n",
    "\n",
    "\n",
    "### mRNA/lncRNA Classification\n",
    "\n",
    "These results show a direct comparison to [27] using data from the paper. The classification dataset consists of DNA sequences corresponding to mRNA and lncRNA sequences. The dataset contains two test sets - a standard test set and a challenge test set. In the table below, results from a single Genomic-ULMFiT model are compared to an ensemble of GRU models used by [27].\n",
    "\n",
    "\n",
    "| Model                          \t| Test Set           \t| Accuracy \t| Specificity \t| Sensitivity \t| Precision \t| MCC   \t|\n",
    "|--------------------------------\t|--------------------\t|----------\t|-------------\t|-------------\t|-----------\t|-------\t|\n",
    "| GRU Ensemble (Hill et al.)*    \t| Standard Test Set  \t|   0.96   \t|     __0.97__    \t|     0.95    \t|    __0.97__   \t|  0.92 \t|\n",
    "| Genomic-ULMFiT \t| Standard Test Set  \t|   __0.963__  \t|    0.952    \t|    __0.974__    \t|   0.953   \t| __0.926__ \t|\n",
    "| GRU Ensemble (Hill et al.)*    \t| Challenge Test Set \t|   0.875  \t|     __0.95__    \t|     0.80    \t|    __0.95__   \t|  0.75 \t|\n",
    "| Genomic-ULMFiT \t| Challenge Test Set \t|   __0.90__   \t|    0.944    \t|    __0.871__    \t|   0.939   \t| __0.817__ \t|\n",
    "\n",
    "(*) [27] presented their results as a plot rather than as a data table. Values in the above table are estimated by reading off the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "### Effect of k-mer and Stride Tokenization Parameters\n",
    "\n",
    "Tokenizing genomic sequences with different k-mer and stride values impact classification performance. Different datasets respond differently to changes in k-mer and stride. For example, with the metagenomics dataset from [10]:\n",
    "\n",
    "| Amplicon Data   \t| kmer/stride \t| Accuracy \t| Precision \t| Recall \t| F1    \t|\n",
    "|-----------------\t|-------------\t|----------\t|-----------\t|--------\t|-------\t|\n",
    "| Genomic-ULMFiT  \t|     5/2     \t|   .9144  \t|   .9369   \t|  .9250 \t| .9214 \t|\n",
    "| Genomic-ULMFiT  \t|     5/1     \t|   .9150  \t|   .9309   \t|  .9263 \t| .9230 \t|\n",
    "| Genomic-ULMFiT  \t|     3/1     \t|   __.9239__  \t|   __.9402__   \t|  __.9332__ \t| __.9306__ \t|\n",
    "\n",
    "| Shotgun Data    \t| kmer/stride \t| Accuracy \t| Precision \t| Recall \t| F1    \t|\n",
    "|-----------------\t|-------------\t|----------\t|-----------\t|--------\t|-------\t|\n",
    "| Genomic-ULMFiT  \t|     5/2     \t|   .8075  \t|   .8102   \t|  .8054 \t| .8044 \t|\n",
    "| Genomic-ULMFiT  \t|     5/1     \t|   .8528  \t|   .8631   \t|  .8566 \t| .8569 \t|\n",
    "| Genomic-ULMFiT  \t|     3/1     \t|   __.8797__  \t|   __.8824__   \t|  __.8769__ \t| __.8758__ \t|\n",
    "\n",
    "For both the shotgun and amplicon datasets, performance improved by using smaller k-mer and stride values. However for the amplicon dataset, accuracy improved by ~1%, while for the shotgun dataset, accuracy improved by ~7%.\n",
    "\n",
    "For the short promoter dataset from [5]:\n",
    "\n",
    "| kmer/stride \t| Accuracy \t| Precision \t| Recall \t| Correlation Coefficient \t| Specificity \t|\n",
    "|-------------\t|----------\t|-----------\t|--------\t|-------------------------\t|-------------\t|\n",
    "|     5/2     \t|   .977   \t|    .959   \t|  .989  \t|           .955          \t|     .969    \t|\n",
    "|     5/1     \t|   .990   \t|    .983   \t|  .995  \t|           .981          \t|     .987    \t|\n",
    "|     3/1     \t|   __.995__   \t|    __.992__   \t|  __.996__  \t|           __.991__          \t|     __.994__    \t|\n",
    "\n",
    "This dataset shows a slight improvement using a 3/1 k-mer/stride scheme over 5/1.\n",
    "\n",
    "For the long promoter dataset from [6]:\n",
    "\n",
    "| Kmer/Stride \t| Accuracy \t| Precision \t| Recall \t| Correlation Coefficient \t|\n",
    "|-------------\t|----------\t|-----------\t|--------\t|-------------------------\t|\n",
    "|     5/2     \t|   0.889  \t|   0.886   \t|  0.846 \t|          0.772          \t|\n",
    "|     4/2     \t|   0.892  \t|   0.877   \t|  __0.865__ \t|          0.778          \t|\n",
    "|     8/3     \t|   0.874  \t|   0.889   \t|  0.802 \t|          0.742          \t|\n",
    "|     1/1     \t|   __0.894__  \t|   0.900   \t|  0.844 \t|          __0.784__          \t|\n",
    "\n",
    "This dataset shows a slight accuracy improvment with a 1/1 scheme over a 4/2 scheme, but not by much. Precision and recall for the 4/1 and 1/1 models show tradeoffs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "[1] Jeremy Howard and Sebastian Ruder. 2018. Universal Language Model Fine-tuning for Text Classification. arXiv preprint arXiv:1801.06146\n",
    "\n",
    "[2] Chuai G, Ma H, Yan J, et al. DeepCRISPR: optimized CRISPR guide RNA design by deep learning. Genome Biol. 2018;19(1):80. Published 2018 Jun 26. doi:10.1186/s13059-018-1459-4\n",
    "\n",
    "[3] Min X, Zeng W, Chen S, Chen N, Chen T, Jiang R. Predicting enhancers with deep convolutional neural networks. BMC Bioinformatics. 2017;18(Suppl 13):478. Published 2017 Dec 1. doi:10.1186/s12859-017-1878-3\n",
    "\n",
    "[4] Bite Yang, Feng Liu, Chao Ren, Zhangyi Ouyang, Ziwei Xie, Xiaochen Bo, Wenjie Shu, BiRen: predicting enhancers with a deep-learning-based model using the DNA sequence alone, Bioinformatics, Volume 33, Issue 13, 1 July 2017, Pages 1930–1936, https://doi.org/10.1093/bioinformatics/btx105\n",
    "\n",
    "[5] Umarov RK, Solovyev VV (2017) Recognition of prokaryotic and eukaryotic promoters using convolutional deep learning neural networks. PLoS ONE 12(2): e0171410. https://doi.org/10.1371/journal.pone.0171410\n",
    "\n",
    "[6] Umarov RK, et al. 2018. PromID: human promoter prediction by deep learning. arXiv preprint arXiv:1810.01414\n",
    "\n",
    "[7] Cohn D. et al. 2018. Enhancer Identification using Transfer and Adversarial Deep Learning of DNA Sequences. bioRxiv doi:https://doi.org/10.1101/264200\n",
    "\n",
    "[8] Zeng W, Wu M, Jiang R. Prediction of enhancer-promoter interactions via natural language processing. BMC Genomics. 2018;19(Suppl 2):84. Published 2018 May 9. doi:10.1186/s12864-018-4459-6\n",
    "\n",
    "[9] Shen Z, Bao W, Huang DS. Recurrent Neural Network for Predicting Transcription Factor Binding Sites. Sci Rep. 2018;8(1):15270. Published 2018 Oct 15. doi:10.1038/s41598-018-33321-1\n",
    "\n",
    "[10] Fiannaca A, La Paglia L, La Rosa M, et al. Deep learning models for bacteria taxonomic classification of metagenomic data. BMC Bioinformatics. 2018;19(Suppl 7):198. Published 2018 Jul 9. doi:10.1186/s12859-018-2182-6\n",
    "\n",
    "[11] Plekhanova, E., Nuzhdin, S. V., Utkin, L. V., & Samsonova, M. G. ( 2018). Prediction of deleterious mutations in coding regions of mammals with transfer learning. Evolutionary Applications, 12, 18– 28. https://doi.org/10.1111/eva.12607\n",
    "\n",
    "[12] Liu F, Li H, Ren C, Bo X, Shu W. 2016 PEDLA: predicting enhancers with a deep learning-based algorithmic framework. bioRxiv (doi:10.1101/036129) Google Scholar\n",
    "\n",
    "[13] Stephen Merity, Nitish Shirish Keskar, and Richard Socher. 2017. Regularizing and Optimizing LSTM Language Models. arXiv preprint arXiv:1708.02182\n",
    "\n",
    "[14] Sepp Hochreiter and Jürgen Schmidhuber. Long Short-Term memory. Neural computation, 9(8):1735–1780, 1997.\n",
    "\n",
    "[15] Inan, H., Khosravi, K., and Socher, R. Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling. arXiv preprint arXiv:1611.01462, 2016.\n",
    "\n",
    "[16] Press, O. and Wolf, L. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859, 2016.\n",
    "\n",
    "[17] Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15:1929–1958, 2014.\n",
    "\n",
    "[18] Gal, Y. and Ghahramani, Z. A theoretically grounded application of dropout in recurrent neural networks. In NIPS, 2016.\n",
    "\n",
    "[19] Wan, L., Zeiler, M., Zhang, S., LeCun, Y, and Fergus, R. Regularization of neural networks using dropconnect. In Proceedings of the 30th international conference on machine learning (ICML-13), pp. 1058–1066, 2013.\n",
    "\n",
    "[20] Ilya Loshchilov, Frank Hutter. Decoupled Weight Decay Regularization. arXiv preprint arXiv:1711.05101, 2017.\n",
    "\n",
    "[21] Merity, S., McCann, B., and Socher, R. Revisiting activation regularization for language rnns. arXiv preprint arXiv:1708.01009, 2017.\n",
    "\n",
    "[22] Leslie N. Smith. A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay. arXiv preprint arXiv:1803.09820, 2018\n",
    "\n",
    "[23] Leslie N. Smith, Nicholay Topin. Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates. arXiv preprint arXiv:1708.07120, 2017\n",
    "\n",
    "[24] Leslie N Smith. Cyclical Learning Rates for Training Neural Networks. arXiv preprint arXiv:1506.01186, 2015\n",
    "\n",
    "[25] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. How transferable are features in deep neural networks? In Advances in neural information processing systems. pages 3320–3328.\n",
    "\n",
    "[26] Tianwei Yue, Haohan Wang. Deep Learning for Genomics: A Concise Overview. arXiv preprint arXiv:1802.00810, 2018\n",
    "\n",
    "[27] Hill S.T., Kuintzle R., Teegarden A., Merrill E., 3rd, Danaee P., Hendrix D.A. A deep recurrent neural network discovers complex biological rules to decipher RNA protein-coding potential. Nucleic Acids Res. 2018;46:8105–8113. doi: 10.1093/nar/gky567.\n",
    "\n",
    "[28] Weihua Guo, You Xu, Xueyang Feng. DeepMetabolism: A Deep Learning System to Predict Phenotype from Genome Sequencing. arXiv preprint arXiv:1705.03094, 2017.\n",
    "\n",
    "[29] Seonwoo Min, Byunghan Lee, Sungroh Yoon. Deep Learning in Bioinformatics. arXiv preprint arXiv:1603.06430, 2016.\n",
    "\n",
    "[30] Young J.D., Cai C., Lu X. Unsupervised deep learning reveals prognostically relevant subtypes of glioblastoma. BMC Bioinform. 2017;18:381. doi: 10.1186/s12859-017-1798-2\n",
    "\n",
    "[31] Yu Li et al. Deep learning in bioinformatics: introduction, application, and perspective in big data era. arXiv preprint arXiv:1903.00342 2019\n",
    "\n",
    "[32] Roy AL, Singer DS. Core promoters in transcription: old problem, new insights. Trends Biochem. Sci. 2015;40:165–171. doi: 10.1016/j.tibs.2015.01.007."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
